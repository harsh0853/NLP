{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d0b4a6-0394-4ba1-9d11-e9e409ffdceb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**NLP Concepts Revision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4a7219d-0d17-4302-af05-8ae6a12edfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"Hey , Harsh Harsh this side.\"), Sentence(\"Today's weather is very bad bad bad bad bad and cold\")]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer \n",
    "text = \"Hey , Harsh Harsh this side. Today's weather is very bad bad bad bad bad and cold\"\n",
    "blob = TextBlob(text , analyzer=NaiveBayesAnalyzer())\n",
    "print(blob.sentences) #Tokenises into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a80329b8-9032-4f1c-b955-d79fbf95bbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Hey', 'Harsh', 'Harsh', 'this', 'side', 'Today', \"'s\", 'weather', 'is', 'very', 'bad', 'bad', 'bad', 'bad', 'bad', 'and', 'cold'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words #Tokenises into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e4fcdb1-f20c-4121-9dda-d3c99d31fb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'NNP'),\n",
       " ('Harsh', 'NNP'),\n",
       " ('Harsh', 'NNP'),\n",
       " ('this', 'DT'),\n",
       " ('side', 'NN'),\n",
       " ('Today', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('weather', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('very', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('bad', 'JJ'),\n",
       " ('bad', 'JJ'),\n",
       " ('bad', 'JJ'),\n",
       " ('bad', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('cold', 'JJ')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags #Determines POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c8ae4ba-c87a-4817-bef6-6ac49874f235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['hey', 'harsh harsh', \"'s weather\"])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases #returns noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a7996724-3168-4349-8e64-74b0b601faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='pos', p_pos=0.575582350078385, p_neg=0.4244176499216147)\n",
      "Sentiment(classification='neg', p_pos=0.4685255220434274, p_neg=0.5314744779565722)\n"
     ]
    }
   ],
   "source": [
    "blob.sentiment #provides us with the sentiments\n",
    "#this one was from pattern analyzer\n",
    "#By default it uses PatternAnalyzer\n",
    "for s in blob.sentences:\n",
    "    print(s.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a8e326d-3f30-452b-bee0-907e90117f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(classification='pos', p_pos=0.575582350078385, p_neg=0.4244176499216147)\n",
      "Sentiment(classification='neg', p_pos=0.4685255220434274, p_neg=0.5314744779565722)\n"
     ]
    }
   ],
   "source": [
    "#NaiveBayes sentiment analysis from textblob.sentiments module\n",
    "for s in blob.sentences:\n",
    "    print(s.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff1b2726-0a9f-4067-ba27-686321b5d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heyaaa , Wish by\n"
     ]
    }
   ],
   "source": [
    "texti = TextBlob(\"Heyaaa , Hrsh bd\")\n",
    "for s in texti.sentences:\n",
    "    print(s.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c4bd05b2-1356-4b33-a483-283b84210a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varieti\n",
      "variety\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "textt = Word(\"varieties\")\n",
    "print(textt.stem())\n",
    "print(textt.lemmatize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b166dd0-9c58-4db8-96e0-5026b48788a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"The Rise of Artificial Intelligence in Everyday Life\n",
    "\n",
    "Artificial Intelligence (AI) is no longer a futuristic concept confined to science fiction; it has seamlessly integrated into our everyday lives. From voice assistants like Siri and Alexa to recommendation engines on Netflix and Amazon, AI is quietly shaping the way we interact with technology. One of the most transformative applications is in healthcare, where AI-powered diagnostics help detect diseases such as cancer earlier and more accurately than ever before. Similarly, in transportation, self-driving cars and AI-assisted traffic management systems are beginning to redefine mobility. Even in the realm of education, personalized learning platforms use AI to adapt content to suit individual students' needs and learning speeds. While the benefits of AI are enormous, its rapid growth also raises ethical questions. Concerns about privacy, data security, job displacement, and decision-making biases highlight the need for thoughtful regulation and responsible AI development. Nevertheless, the potential for AI to improve lives, solve complex problems, and increase efficiency is undeniable. As the technology continues to evolve, fostering a balance between innovation and responsibility will be key to harnessing AI’s power for the greater good. The future of AI lies not in replacing humans, but in augmenting human potential across every facet of society.\"\"\"\n",
    "\n",
    "t = TextBlob(text)\n",
    "\n",
    "t.words.count(\"is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "187aec54-39bf-4051-97e5-3355c377a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "34718409-8201-4286-af8d-c756be7e21bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence (or 'quit' to exit):  articicial intelligence is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word: transforming\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence (or 'quit' to exit):  quit\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "while True:\n",
    "    a = input(\"Enter text (or 'n' to exit): \")\n",
    "    if a.lower() == 'n':\n",
    "        break\n",
    "\n",
    "    ta = TextBlob(a)\n",
    "\n",
    "    try:\n",
    "        # Change '2' to 'n' if you want trigrams or higher\n",
    "        x = ta.ngrams(n=2)  # Generating bigrams\n",
    "\n",
    "        if x:\n",
    "            print(\"Second word of first bigram:\", x[0][1])\n",
    "        else:\n",
    "            print(\"Not enough words for a bigram.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa3f35-2b29-4da2-adfa-709e8e1e8fdf",
   "metadata": {},
   "source": [
    "**There are many formulas used in natural language processing to calculate readability. Textatistic uses five popular readability formulas—Flesch Reading Ease, Flesch-Kincaid, Gunning Fog, Simple Measure of Gobbledygook (SMOG) and Dale-Chall.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6fdb9b52-fc22-4e30-b1d1-313042a7faa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_count': 34,\n",
       " 'word_count': 8,\n",
       " 'sent_count': 2,\n",
       " 'sybl_count': 9,\n",
       " 'notdalechall_count': 2,\n",
       " 'polysyblword_count': 0,\n",
       " 'flesch_score': 107.60000000000001,\n",
       " 'fleschkincaid_score': -0.754999999999999,\n",
       " 'gunningfog_score': 1.6,\n",
       " 'smog_score': 3.1291,\n",
       " 'dalechall_score': 7.7824}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "text = \"Hey harsh this side. How are ya'll doing?\"\n",
    "r = Textatistic(text)\n",
    "r.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "696be4a8-344f-4b45-ac44-be08e6621726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_15796\\726956180.py:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  d1.similarity(d2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8676270638597254"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "d1 = nlp(\"Hey this is Harsh\")\n",
    "d2 = nlp(\"Hey this is Hash\")\n",
    "d1.similarity(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda1fb7-a685-45c9-a7b8-62c82a632f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
